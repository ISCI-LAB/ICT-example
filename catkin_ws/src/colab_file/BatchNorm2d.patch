--- /home/gauravp/anaconda2/envs/pytorch8/lib/python2.7/site-packages/torch/nn/modules/batchnorm.py
+++ /home/gauravp/anaconda2/envs/pytorch8/lib/python2.7/site-packages/torch/nn/modules/batchnorm.py
@@ -1,71 +1,49 @@
 class BatchNorm2d(_BatchNorm):
-    r"""Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
-    with additional channel dimension) as described in the paper
-    `Batch Normalization: Accelerating Deep Network Training by Reducing
-    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
+    r"""Applies Batch Normalization over a 4d input that is seen as a mini-batch
+    of 3d inputs
 
     .. math::
 
-        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
+        y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta
 
     The mean and standard-deviation are calculated per-dimension over
-    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
-    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
-    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
-    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.
+    the mini-batches and gamma and beta are learnable parameter vectors
+    of size C (where C is the input size).
 
-    Also by default, during training this layer keeps running estimates of its
-    computed mean and variance, which are then used for normalization during
-    evaluation. The running estimates are kept with a default :attr:`momentum`
-    of 0.1.
+    During training, this layer keeps a running estimate of its computed mean
+    and variance. The running sum is kept with a default momentum of 0.1.
 
-    If :attr:`track_running_stats` is set to ``False``, this layer then does not
-    keep running estimates, and batch statistics are instead used during
-    evaluation time as well.
+    During evaluation, this running mean/variance is used for normalization.
 
-    .. note::
-        This :attr:`momentum` argument is different from one used in optimizer
-        classes and the conventional notion of momentum. Mathematically, the
-        update rule for running statistics here is
-        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
-        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
-        new observed value.
-
-    Because the Batch Normalization is done over the `C` dimension, computing statistics
-    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.
+    Because the BatchNorm is done over the `C` dimension, computing statistics
+    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm
 
     Args:
-        num_features: :math:`C` from an expected input of size
-            :math:`(N, C, H, W)`
+        num_features: num_features from an expected input of
+            size batch_size x num_features x height x width
         eps: a value added to the denominator for numerical stability.
             Default: 1e-5
         momentum: the value used for the running_mean and running_var
-            computation. Can be set to ``None`` for cumulative moving average
-            (i.e. simple average). Default: 0.1
-        affine: a boolean value that when set to ``True``, this module has
-            learnable affine parameters. Default: ``True``
-        track_running_stats: a boolean value that when set to ``True``, this
-            module tracks the running mean and variance, and when set to ``False``,
-            this module does not track such statistics, and initializes statistics
-            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
-            When these buffers are ``None``, this module always uses batch statistics.
-            in both training and eval modes. Default: ``True``
+            computation. Default: 0.1
+        affine: a boolean value that when set to ``True``, gives the layer learnable
+            affine parameters. Default: ``True``
 
     Shape:
         - Input: :math:`(N, C, H, W)`
         - Output: :math:`(N, C, H, W)` (same shape as input)
 
-    Examples::
-
+    Examples:
         >>> # With Learnable Parameters
         >>> m = nn.BatchNorm2d(100)
         >>> # Without Learnable Parameters
         >>> m = nn.BatchNorm2d(100, affine=False)
-        >>> input = torch.randn(20, 100, 35, 45)
+        >>> input = autograd.Variable(torch.randn(20, 100, 35, 45))
         >>> output = m(input)
     """
 
     def _check_input_dim(self, input):
         if input.dim() != 4:
-            raise ValueError("expected 4D input (got {}D input)".format(input.dim()))
+            raise ValueError('expected 4D input (got {}D input)'
+                             .format(input.dim()))
+        super(BatchNorm2d, self)._check_input_dim(input)
 