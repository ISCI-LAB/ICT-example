--- /home/gauravp/anaconda2/envs/pytorch8/lib/python2.7/site-packages/torch/nn/modules/activation.py
+++ /home/gauravp/anaconda2/envs/pytorch8/lib/python2.7/site-packages/torch/nn/modules/activation.py
@@ -1,45 +1,37 @@
 class Softmax(Module):
     r"""Applies the Softmax function to an n-dimensional input Tensor
     rescaling them so that the elements of the n-dimensional output Tensor
-    lie in the range [0,1] and sum to 1.
+    lie in the range (0,1) and sum to 1
 
-    Softmax is defined as:
-
-    .. math::
-        \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
-
-    When the input Tensor is a sparse tensor then the unspecifed
-    values are treated as ``-inf``.
+    Softmax is defined as
+    :math:`f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}`
 
     Shape:
-        - Input: :math:`(*)` where `*` means, any number of additional
-          dimensions
-        - Output: :math:`(*)`, same shape as the input
+        - Input: any shape
+        - Output: same as input
 
     Returns:
         a Tensor of the same dimension and shape as the input with
         values in the range [0, 1]
 
-    Args:
+    Arguments:
         dim (int): A dimension along which Softmax will be computed (so every slice
             along dim will sum to 1).
 
     .. note::
         This module doesn't work directly with NLLLoss,
         which expects the Log to be computed between the Softmax and itself.
-        Use `LogSoftmax` instead (it's faster and has better numerical properties).
+        Use Logsoftmax instead (it's faster and has better numerical properties).
 
     Examples::
 
-        >>> m = nn.Softmax(dim=1)
-        >>> input = torch.randn(2, 3)
-        >>> output = m(input)
+        >>> m = nn.Softmax()
+        >>> input = autograd.Variable(torch.randn(2, 3))
+        >>> print(input)
+        >>> print(m(input))
+    """
 
-    """
-    __constants__ = ['dim']
-    dim: Optional[int]
-
-    def __init__(self, dim: Optional[int] = None) -> None:
+    def __init__(self, dim=None):
         super(Softmax, self).__init__()
         self.dim = dim
 
@@ -48,9 +40,9 @@
         if not hasattr(self, 'dim'):
             self.dim = None
 
-    def forward(self, input: Tensor) -> Tensor:
+    def forward(self, input):
         return F.softmax(input, self.dim, _stacklevel=5)
 
-    def extra_repr(self) -> str:
-        return 'dim={dim}'.format(dim=self.dim)
+    def __repr__(self):
+        return self.__class__.__name__ + '()'
 